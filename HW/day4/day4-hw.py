import os
import json
import base64
import requests
from typing import List, TypedDict, Literal
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from langgraph.graph import StateGraph, END
from playwright.sync_api import sync_playwright

# --- 1. è¨­å®šå€åŸŸ ---
SEARXNG_URL = "https://puli-8080.huannago.com/search"

# è«‹ç¢ºä¿ API Key æ­£ç¢º
llm = ChatOpenAI(
    base_url="https://ws-05.huannago.com/v1",
    api_key="your_api_key_here", 
    model="google/gemma-3-27b-it",
    temperature=0
)

# --- 2. ç‹€æ…‹å®šç¾© ---
class AgentState(TypedDict):
    question: str
    keywords: str
    knowledge_base: str
    cache_hit: bool
    final_answer: str
    count: int 
    feedback: str  # å„²å­˜ LLM çš„æ€è€ƒåé¥‹

# --- 3. æ ¸å¿ƒå·¥å…·å‡½æ•¸ ---
def search_searxng(query: str, limit: int = 2):
    params = {"q": query, "format": "json", "language": "zh-TW"}
    try:
        response = requests.get(SEARXNG_URL, params=params, timeout=10)
        return [r for r in response.json().get('results', []) if 'url' in r][:limit]
    except Exception as e:
        print(f"âŒ æœå°‹å‡ºéŒ¯: {e}")
        return []

def vlm_analyze_page(url: str, question: str):
    print(f"ğŸ“¸ [VLM] å•Ÿå‹•è¦–è¦ºé–±è®€: {url}")
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page(viewport={'width': 1280, 'height': 800})
            page.goto(url, wait_until="domcontentloaded", timeout=15000)
            page.wait_for_timeout(2000)
            img_b64 = base64.b64encode(page.screenshot()).decode('utf-8')
            browser.close()
            
            msg = HumanMessage(content=[
                {"type": "text", "text": f"åˆ†ææ­¤æˆªåœ–å…§å®¹ä¸¦é‡å°å•é¡Œ '{question}' æä¾›é—œéµè³‡è¨Šã€‚"},
                {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{img_b64}"}}
            ])
            return llm.invoke([msg]).content
    except Exception as e:
        return f"ç¶²é é–±è®€å¤±æ•—: {e}"

# --- 4. LangGraph ç¯€é»å¯¦ä½œ ---
def check_cache(state: AgentState):
    print("\n[Node] 1. æª¢æŸ¥å¿«å–...")
    return {"cache_hit": False, "knowledge_base": "", "count": 0, "feedback": ""}

def query_gen(state: AgentState):
    new_count = state.get("count", 0) + 1
    fb = f"\nå‰æ¬¡æ€è€ƒåé¥‹ï¼š{state['feedback']}" if state['feedback'] else ""
    print(f"ğŸ”„ [Node] 2. ç¬¬ {new_count}/3 æ¬¡æœå°‹ - ç”Ÿæˆé—œéµå­—...")
    
    prompt = f"å•é¡Œï¼š'{state['question']}'{fb}\nè«‹ç”¢å‡ºä¸€å€‹ç²¾æº–çš„æœå°‹é—œéµå­—ï¼ˆåƒ…è¼¸å‡ºå­—ä¸²å…§å®¹ï¼‰ã€‚"
    keyword = llm.invoke(prompt).content.strip().replace('"', '')
    return {"keywords": keyword, "count": new_count}

def search_tool(state: AgentState):
    print(f"ğŸ” [Node] 3. åŸ·è¡Œæª¢ç´¢: {state['keywords']}")
    results = search_searxng(state['keywords'])
    info = ""
    for r in results:
        analysis = vlm_analyze_page(r['url'], state['question'])
        info += f"\n[ä¾†æº: {r['title']}]\n{analysis}\n"
    return {"knowledge_base": state['knowledge_base'] + info}

def planner(state: AgentState):
    print(f"ğŸ§  [Node] 4. Planner æ·±åº¦æ€è€ƒä¸­...")
    prompt = f"""
    è«‹è©•ä¼°ç¾æœ‰è³‡è¨Šæ˜¯å¦è¶³ä»¥å›ç­”å•é¡Œã€‚
    å•é¡Œï¼š{state['question']}
    ç¾æœ‰è³‡è¨Šï¼š{state['knowledge_base']}
    
    è«‹ä»¥ JSON æ ¼å¼å›å‚³ï¼š
    {{
        "sufficient": "YES" æˆ– "NO",
        "feedback": "è‹¥ç‚º NOï¼Œè«‹èªªæ˜é‚„ç¼ºå°‘ä»€éº¼è³‡è¨Šï¼Ÿè‹¥ç‚º YESï¼Œè«‹å¡«å¯« OK"
    }}
    """
    res = llm.invoke(prompt).content
    try:
        # ç°¡å–®è§£æ JSON å…§å®¹
        data = json.loads(res[res.find("{"):res.rfind("}")+1])
        decision = data.get("sufficient", "NO")
        feedback = data.get("feedback", "è³‡è¨Šä»ä¸è¶³")
    except:
        decision = "NO"
        feedback = "ç„¡æ³•è§£ææ€è€ƒå…§å®¹ï¼Œå»ºè­°æ“´å¤§æœå°‹ç¯„åœ"

    print(f"ğŸ¤” æ±ºç­–ï¼š{decision} | åé¥‹ï¼š{feedback}")
    return {"feedback": feedback, "final_answer": decision}

def final_answer(state: AgentState):
    print("ğŸ“¢ [Node] 5. ç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ...")
    prompt = f"æ ¹æ“šä»¥ä¸‹è³‡è¨Šï¼Œç‚ºä½¿ç”¨è€…æä¾›å®Œæ•´ä¸”å°ˆæ¥­çš„æŸ¥è­‰å ±å‘Šï¼š\n{state['knowledge_base']}\nå•é¡Œï¼š{state['question']}"
    res = llm.invoke(prompt).content
    return {"final_answer": res}

# --- 5. æ§‹å»ºæµç¨‹åœ– ---
workflow = StateGraph(AgentState)

# ç‚ºäº†è®“ ASCII å‘ˆç¾ç‰¹å®šçš„å¾ªç’°æ¨£å¼ï¼ŒæŒ‰æ­¤é †åºæ·»åŠ ç¯€é»
workflow.add_node("check_cache", check_cache)
workflow.add_node("planner", planner)
workflow.add_node("final_answer", final_answer)
workflow.add_node("query_gen", query_gen)
workflow.add_node("search_tool", search_tool)

workflow.set_entry_point("check_cache")

# è¨­å®šè·¯å¾‘
workflow.add_conditional_edges(
    "check_cache",
    lambda x: "final_answer" if x["cache_hit"] else "query_gen",
    {"final_answer": "final_answer", "query_gen": "query_gen"}
)

workflow.add_edge("query_gen", "search_tool")
workflow.add_edge("search_tool", "planner")

def route_logic(state: AgentState):
    if state.get("count", 0) >= 3 or "YES" in state.get("final_answer", ""):
        return "final_answer"
    return "query_gen" # å¸¶è‘—åé¥‹å›åˆ°ç”Ÿæˆé—œéµå­—

workflow.add_conditional_edges(
    "planner",
    route_logic,
    {"final_answer": "final_answer", "query_gen": "query_gen"}
)

workflow.add_edge("final_answer", END)
app = workflow.compile()

# --- 6. è¼¸å‡ºåœ–è¡¨èˆ‡åŸ·è¡Œ ---
print("\n" + "="*20 + " ç³»çµ±æµç¨‹åœ– " + "="*20)
app.get_graph().print_ascii()
print("="*55 + "\n")

if __name__ == "__main__":
    q = input("è«‹è¼¸å…¥æŸ¥è­‰å•é¡Œï¼š")
    for output in app.stream({"question": q, "knowledge_base": "", "cache_hit": False, "count": 0}):
        for node, data in output.items():
            if "final_answer" in data and node == "final_answer":
                print("\n" + "âœ¨"*10 + " æŸ¥è­‰å ±å‘Š " + "âœ¨"*10)
                print(data["final_answer"])