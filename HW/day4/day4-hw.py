import os
import json
import time
from typing import TypedDict, List, Literal
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from langgraph.graph import StateGraph, END

# --- åŒ¯å…¥åƒè€ƒæª”æ¡ˆåŠŸèƒ½ (è«‹ç¢ºä¿æª”æ¡ˆåœ¨åŒç›®éŒ„) ---
try:
    from search_searxng import search_searxng
    from vlm_read_website import vlm_read_website
except ImportError as e:
    print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°å¿…è¦çš„å·¥å…·æª”æ¡ˆ ({e})ã€‚è«‹ç¢ºèª search_searxng.py èˆ‡ vlm_read_website.py åœ¨åŒç›®éŒ„ä¸‹ã€‚")

# --- 1. å®šç¾©ç’°å¢ƒèˆ‡æ¨¡å‹ ---
llm = ChatOpenAI(
    base_url="https://ws-02.wade0426.me/v1",
    api_key="", # âš ï¸ è«‹åœ¨æ­¤å¡«å…¥æ‚¨çš„ API Key
    model="google/gemma-3-27b-it",
    temperature=0.7
)

CACHE_FILE = "qa_cache.json"

# --- 2. å®šç¾©ç‹€æ…‹ (State) ---
class State(TypedDict):
    input_query: str          # ä½¿ç”¨è€…åŸå§‹è¼¸å…¥
    knowledge_base: str       # ç´¯ç©çš„æŸ¥è­‰è³‡è¨Š
    keywords: List[str]       # ç”Ÿæˆçš„é—œéµå­—
    search_links: List[dict]  # æª¢ç´¢çµæœ
    final_answer: str         # æœ€çµ‚å›ç­”
    is_sufficient: bool       # LLM åˆ¤æ–·è³‡è¨Šæ˜¯å¦è¶³å¤ 
    loop_count: int           # å¾ªç’°æ¬¡æ•¸è¨ˆæ•¸å™¨

# --- 3. å®šç¾©ç¯€é» (Nodes) ---

def check_cache_node(state: State):
    """æª¢æŸ¥å¿«å–ï¼Œè‹¥å‘½ä¸­å‰‡ç›´æ¥è³¦å€¼ final_answer"""
    print("\n--- [ç¯€é»] å¿«å–æª¢æŸ¥ ---")
    clean_key = state['input_query'].replace(" ", "").replace("?", "")
    
    # é è¨­åˆå§‹åŒ–ç‹€æ…‹
    init_state = {
        "knowledge_base": "", 
        "loop_count": 0, 
        "final_answer": "", 
        "is_sufficient": False
    }
    
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, "r", encoding="utf-8") as f:
                cache = json.load(f)
                if clean_key in cache:
                    print(">>> âœ… Hit: å‘½ä¸­å¿«å–ï¼Œç›´æ¥è·³è‡³è¼¸å‡º")
                    init_state["final_answer"] = cache[clean_key]
                    return init_state
        except: pass
    
    print(">>> âŒ Miss: ç„¡å¿«å–ç´€éŒ„ï¼Œé€²å…¥æœå°‹æµç¨‹")
    return init_state

def planner_node(state: State):
    """æ±ºç­–ç¯€é»ï¼šåˆ¤æ–·è³‡è¨Šæ˜¯å¦è¶³å¤ ï¼Œæˆ–æ˜¯å¦é”åˆ°æœ€å¤§å¾ªç’°"""
    count = state.get('loop_count', 0)
    print(f"--- [ç¯€é»] æ±ºç­– (Planner) | ç•¶å‰å¾ªç’°æ¬¡æ•¸: {count} ---")
    
    # å¼·åˆ¶é™åˆ¶ï¼šè‹¥å·²é” 3 æ¬¡å¾ªç’°ï¼Œå¼·åˆ¶è¦–ç‚ºå……è¶³ï¼Œåœæ­¢æœå°‹
    if count >= 3:
        print("âš ï¸ è­¦å‘Šï¼šå·²é”åˆ°æœ€å¤§æœå°‹æ¬¡æ•¸ (3æ¬¡)ï¼Œæº–å‚™å½™æ•´ç¾æœ‰è³‡è¨Šã€‚")
        return {"is_sufficient": True}

    prompt = f"""
    ä½¿ç”¨è€…å•é¡Œ: {state['input_query']}
    ç›®å‰æŒæ¡è³‡è¨Š: {state['knowledge_base'] if state['knowledge_base'] else 'ç›®å‰å°šæœªå–å¾—ç¶²é è³‡è¨Š'}
    
    è«‹è©•ä¼°ç›®å‰è³‡è¨Šæ˜¯å¦è¶³ä»¥å›ç­”å•é¡Œï¼Ÿ
    å›ç­”è¦æ±‚ï¼šåƒ…éœ€å›ç­” 'y' (å……è¶³) æˆ– 'n' (ä¸è¶³)ã€‚
    """
    response = llm.invoke([HumanMessage(content=prompt)])
    is_y = "y" in response.content.lower()
    return {"is_sufficient": is_y}

def query_gen_node(state: State):
    """ç”Ÿæˆé—œéµå­—ç¯€é»"""
    print("--- [ç¯€é»] ç”Ÿæˆæœå°‹é—œéµå­— ---")
    prompt = f"é‡å°å•é¡Œ '{state['input_query']}'ï¼Œè«‹ç”Ÿæˆä¸€å€‹æœ€é©åˆåœ¨æœå°‹å¼•æ“æŸ¥æ‰¾çš„é—œéµå­—ï¼Œä¸è¦æœ‰å»¢è©±ã€‚"
    response = llm.invoke([HumanMessage(content=prompt)])
    return {"keywords": [response.content.strip()]}

def search_tool_node(state: State):
    """åŸ·è¡Œç¶²é æª¢ç´¢"""
    keyword = state['keywords'][0]
    print(f"--- [ç¯€é»] åŸ·è¡Œ SearXNG æª¢ç´¢: {keyword} ---")
    results = search_searxng(keyword, limit=1) 
    return {"search_links": results}

def vlm_node(state: State):
    """VLM è¦–è¦ºè™•ç†ç¯€é»ï¼Œä¸¦ç´¯åŠ è¨ˆæ•¸å™¨"""
    print("--- [ç¯€é»] VLM è¦–è¦ºé–±è®€ç¶²é  ---")
    current_count = state.get("loop_count", 0)
    
    if not state.get('search_links'):
        return {"knowledge_base": state['knowledge_base'] + "\næœªæ‰¾åˆ°ç›¸é—œé€£çµã€‚", "loop_count": current_count + 1}
    
    target = state['search_links'][0]
    # èª¿ç”¨ Playwright æˆªåœ–èˆ‡ VLM åˆ†æ
    summary = vlm_read_website(target['url'], target['title'])
    
    new_kb = f"{state['knowledge_base']}\n\n[ä¾†æº: {target['url']}]\n{summary}"
    return {"knowledge_base": new_kb, "loop_count": current_count + 1}

def output_node(state: State):
    """å½™æ•´è³‡è¨Šç”¢ç”Ÿæœ€çµ‚å›ç­”ï¼Œä¸¦å­˜å…¥å¿«å–"""
    print("--- [ç¯€é»] ç”¢ç”Ÿæœ€çµ‚è¼¸å‡º ---")
    
    # å¦‚æœæ˜¯å¿«å–å‘½ä¸­çš„ï¼Œç›´æ¥çµæŸ
    if state.get("final_answer"):
        return {"final_answer": state["final_answer"]}

    prompt = f"è«‹æ ¹æ“šä»¥ä¸‹æŸ¥è­‰è³‡è¨Šï¼Œå®Œæ•´ä¸”ç²¾ç¢ºåœ°å›ç­”ä½¿ç”¨è€…å•é¡Œ '{state['input_query']}':\n{state['knowledge_base']}"
    response = llm.invoke([HumanMessage(content=prompt)])
    
    # å¯«å…¥å¿«å–
    clean_key = state['input_query'].replace(" ", "").replace("?", "")
    cache_data = {}
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, "r", encoding="utf-8") as f: cache_data = json.load(f)
        except: pass
    
    cache_data[clean_key] = response.content
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(cache_data, f, ensure_ascii=False, indent=4)
        
    return {"final_answer": response.content}

# --- 4. æ§‹å»ºåœ–è¡¨ (Graph) ---

def cache_router(state: State):
    return "hit" if state.get("final_answer") else "miss"

def planner_router(state: State):
    # å¦‚æœå¾ªç’°æ¬¡æ•¸ >= 3ï¼Œç„¡è«– LLM åˆ¤æ–·ç‚ºä½•ï¼Œä¸€å¾‹å» output
    if state.get("loop_count", 0) >= 3:
        return "y"
    return "y" if state.get("is_sufficient") else "n"

workflow = StateGraph(State)

workflow.add_node("check_cache", check_cache_node)
workflow.add_node("planner", planner_node)
workflow.add_node("query_gen", query_gen_node)
workflow.add_node("search_tool", search_tool_node)
workflow.add_node("vlm_process", vlm_node)
workflow.add_node("output", output_node)

workflow.set_entry_point("check_cache")

# è¨­å®šé€£ç·šé‚è¼¯
workflow.add_conditional_edges("check_cache", cache_router, {"hit": "output", "miss": "planner"})
workflow.add_conditional_edges("planner", planner_router, {"y": "output", "n": "query_gen"})

workflow.add_edge("query_gen", "search_tool")
workflow.add_edge("search_tool", "vlm_process")
workflow.add_edge("vlm_process", "planner") # å¾ªç’°é»
workflow.add_edge("output", END)

app = workflow.compile()

# --- 5. åŸ·è¡Œå€ (äº’å‹•ä»‹é¢ç‰ˆ) ---
if __name__ == "__main__":
    print("\n" + "========================================")
    print("  ğŸš€ è‡ªå‹•æŸ¥è­‰ AI ç³»çµ± (Gemma-3 VLM) å·²å•Ÿå‹•")
    print("  (è¼¸å…¥ 'exit' æˆ– 'é›¢é–‹' çµæŸç¨‹å¼)")
    print("========================================\n")

    while True:
        query = input("â“ è«‹è¼¸å…¥æ‚¨æƒ³æŸ¥è©¢çš„å•é¡Œï¼š").strip()

        if query.lower() in ['exit', 'quit', 'é›¢é–‹', 'é€€å‡º']:
            print("ğŸ‘‹ ç¨‹å¼å·²å®‰å…¨é€€å‡ºï¼Œå†è¦‹ï¼")
            break
        
        if not query:
            continue

        print(f"\nâš™ï¸ æ­£åœ¨è™•ç†è«‹æ±‚ï¼Œè«‹ç¨å€™...")
        
        try:
            # å•Ÿå‹• LangGraph
            final_state = app.invoke({
                "input_query": query, 
                "knowledge_base": "", 
                "loop_count": 0, 
                "final_answer": ""
            })
            
            print("\n" + "âœ¨" + "â€”"*48)
            print(f"ã€æœ€çµ‚å›ç­”ã€‘\n\n{final_state['final_answer']}")
            print("â€”"*50 + "\n")
            
        except Exception as e:
            print(f"âŒ ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")