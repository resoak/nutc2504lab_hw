import os
import uuid
import requests
import pandas as pd
import torch
import numpy as np
from pathlib import Path
from PIL import Image
import re

# --- ç’°å¢ƒè¨­å®š ---
BASE_CACHE = "C:/huggingface_cache"
os.makedirs(BASE_CACHE, exist_ok=True)
os.environ['HF_HOME'] = BASE_CACHE
os.environ['DOCLING_CACHE_DIR'] = BASE_CACHE
os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'

# --- Docling ç›¸é—œå°å…¥ ---
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import (
    VlmPipelineOptions, 
    PdfPipelineOptions, 
    RapidOcrOptions
)
from docling.datamodel.pipeline_options_vlm_model import ApiVlmOptions, ResponseFormat
from docling.document_converter import DocumentConverter, PdfFormatOption, ImageFormatOption
from docling.pipeline.vlm_pipeline import VlmPipeline

# --- LLM Guard / RAG / è©•æ¸¬å°å…¥ ---
from llm_guard.input_scanners import PromptInjection
from llm_guard.input_scanners.prompt_injection import MatchType
from qdrant_client import QdrantClient, models
from transformers import AutoTokenizer, AutoModelForCausalLM
from deepeval.metrics import (
    FaithfulnessMetric, 
    AnswerRelevancyMetric, 
    ContextualRecallMetric, 
    ContextualPrecisionMetric
)
from deepeval.test_case import LLMTestCase
from deepeval.models.base_model import DeepEvalBaseLLM
from openai import OpenAI

# --- 1. é…ç½®å€åŸŸ ---
VLLM_URL = "https://ws-01.wade0426.me/v1/chat/completions"
EMBED_URL = "https://ws-04.wade0426.me/embed"
CHAT_API_URL = "https://ws-03.wade0426.me/v1"
JUDGE_MODEL_ID = "/models/Qwen3-30B-A3B-Instruct-2507-FP8"
COLLECTION_NAME = "final_hybrid_rag_stable"
RERANKER_PATH = r"C:\Users\RS\Downloads\Qwen3-Reranker-0.6B" 
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BATCH_SIZE = 2

# Token ç®¡ç†é…ç½®
MAX_CONTEXT_CHARS = 6000  # å­—å…ƒé™åˆ¶ï¼ˆç´„ 2000-2500 tokensï¼‰
MAX_CHUNK_SIZE = 500  # æ¯å€‹èªæ„å¡Šçš„æœ€å¤§å­—å…ƒæ•¸
MAX_IMAGE_PIXELS = 800 * 800

# --- 2. èªæ„åˆ†å¡Šå·¥å…· ---
class SemanticChunker:
    """
    åŸºæ–¼èªæ„çš„æ–‡æœ¬åˆ†å¡Šå™¨
    """
    def __init__(self, max_chunk_size=MAX_CHUNK_SIZE):
        self.max_chunk_size = max_chunk_size
    
    def split_by_semantics(self, text):
        """
        æŒ‰ç…§èªæ„é‚Šç•Œåˆ†å‰²æ–‡æœ¬
        å„ªå…ˆç´š: æ®µè½ > å¥å­ > å›ºå®šé•·åº¦
        """
        if not text or len(text) < self.max_chunk_size:
            return [text] if text else []
        
        chunks = []
        
        # ç¬¬ä¸€æ­¥: æŒ‰æ®µè½åˆ†å‰²ï¼ˆç©ºè¡Œæˆ–æ›è¡Œç¬¦ï¼‰
        paragraphs = re.split(r'\n\s*\n|\n{2,}', text)
        
        current_chunk = ""
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
            
            # å¦‚æœç•¶å‰æ®µè½æœ¬èº«å¤ªé•·ï¼Œéœ€è¦é€²ä¸€æ­¥åˆ†å‰²
            if len(para) > self.max_chunk_size:
                # å…ˆä¿å­˜ç•¶å‰ç´¯ç©çš„å¡Š
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = ""
                
                # æŒ‰å¥å­åˆ†å‰²é•·æ®µè½
                sentences = self._split_sentences(para)
                for sent in sentences:
                    if len(current_chunk) + len(sent) > self.max_chunk_size:
                        if current_chunk:
                            chunks.append(current_chunk.strip())
                        current_chunk = sent
                    else:
                        current_chunk += sent + " "
            else:
                # æ­£å¸¸æ®µè½ï¼Œå˜—è©¦åˆä½µ
                if len(current_chunk) + len(para) > self.max_chunk_size:
                    chunks.append(current_chunk.strip())
                    current_chunk = para
                else:
                    current_chunk += "\n" + para if current_chunk else para
        
        # ä¿å­˜æœ€å¾Œçš„å¡Š
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def _split_sentences(self, text):
        """
        æŒ‰å¥å­åˆ†å‰²æ–‡æœ¬
        """
        # ä¸­è‹±æ–‡å¥å­é‚Šç•Œ
        pattern = r'([ã€‚ï¼ï¼Ÿ\.!?]+[\s"\'ï¼‰ã€‘]*)'
        sentences = re.split(pattern, text)
        
        # é‡çµ„å¥å­ï¼ˆåŒ…å«æ¨™é»ï¼‰
        result = []
        for i in range(0, len(sentences)-1, 2):
            sent = sentences[i]
            if i+1 < len(sentences):
                sent += sentences[i+1]
            if sent.strip():
                result.append(sent.strip())
        
        # è™•ç†æœ€å¾Œä¸€å€‹å¯èƒ½æ²’æœ‰æ¨™é»çš„å¥å­
        if len(sentences) % 2 == 1 and sentences[-1].strip():
            result.append(sentences[-1].strip())
        
        return result
    
    def chunk_document(self, document_text):
        """
        å°‡æ•´å€‹æ–‡æª”åˆ†å¡Š
        """
        chunks = self.split_by_semantics(document_text)
        
        print(f"  ğŸ“¦ æ–‡æª”åˆ†å¡Š: {len(document_text)} å­—å…ƒ â†’ {len(chunks)} å€‹èªæ„å¡Š")
        
        return chunks

# --- 3. åœ–ç‰‡é è™•ç†å·¥å…· ---
def resize_image_if_needed(img_path, max_pixels=MAX_IMAGE_PIXELS):
    """
    å¦‚æœåœ–ç‰‡å¤ªå¤§ï¼Œèª¿æ•´å¤§å°ä»¥é¿å… VLM token æº¢å‡º
    """
    try:
        img = Image.open(img_path)
        width, height = img.size
        total_pixels = width * height
        
        print(f"  ğŸ“ åœ–ç‰‡å°ºå¯¸: {width}x{height} ({total_pixels:,} åƒç´ )")
        
        if total_pixels > max_pixels:
            ratio = (max_pixels / total_pixels) ** 0.5
            new_width = int(width * ratio)
            new_height = int(height * ratio)
            
            print(f"  ğŸ”„ èª¿æ•´å¤§å°è‡³: {new_width}x{new_height}")
            
            img = img.resize((new_width, new_height), Image.LANCZOS)
            
            temp_path = f"temp_resized_{os.path.basename(img_path)}"
            img.save(temp_path, format=img.format or 'PNG')
            return temp_path
        
        return img_path
    except Exception as e:
        print(f"  âš ï¸  åœ–ç‰‡é è™•ç†å¤±æ•—: {e}")
        return img_path

# --- 4. æ”¹é€²çš„å®‰å…¨æƒæå™¨ ---
class FlexiblePDFScanner:
    """
    æ›´éˆæ´»çš„å®‰å…¨æƒæå™¨
    """
    def __init__(self):
        self.scanner = PromptInjection(threshold=0.95, match_type=MatchType.SENTENCE)
        self.trusted_extensions = ['.docx', '.xlsx', '.pptx']
    
    def scan_content(self, content, file_name):
        print(f"[*] æ­£åœ¨æƒæå®‰å…¨é¢¨éšª: {file_name}")
        
        if not content or len(content) < 100:
            print(f"  â„¹ï¸  å…§å®¹éçŸ­ï¼Œç›´æ¥é€šé")
            return True, 0.0
        
        file_ext = os.path.splitext(file_name)[1].lower()
        
        # ä¿¡ä»»çš„æ–‡æª”é¡å‹ä½¿ç”¨æ›´å¯¬é¬†çš„æª¢æŸ¥
        if file_ext in self.trusted_extensions:
            print(f"  â„¹ï¸  ä¿¡ä»»çš„æ–‡æª”é¡å‹ ({file_ext})ï¼Œä½¿ç”¨å¯¬é¬†æª¢æŸ¥")
            sections = [content[i:i+2000] for i in range(0, min(len(content), 6000), 2000)]
            unsafe_count = 0
            max_risk = 0.0
            
            for idx, s in enumerate(sections[:3]):
                _, is_safe, risk_score = self.scanner.scan(s)
                max_risk = max(max_risk, risk_score)
                if not is_safe:
                    unsafe_count += 1
                    print(f"    âš ï¸  æ®µè½ {idx+1} é¢¨éšªåˆ†æ•¸: {risk_score:.2f}")
            
            if unsafe_count >= 3:
                print(f"  ğŸš¨ æª¢æ¸¬åˆ° {unsafe_count} å€‹é«˜é¢¨éšªæ®µè½")
                return False, max_risk
            
            print(f"  âœ… å®‰å…¨æª¢æŸ¥é€šé (æœ€é«˜é¢¨éšª: {max_risk:.2f})")
            return True, max_risk
        
        # ä¸€èˆ¬æ–‡æª”æª¢æŸ¥
        sections = [content[i:i+1500] for i in range(0, len(content), 1500)]
        max_risk = 0.0
        unsafe_count = 0
        
        for idx, s in enumerate(sections[:5]):
            _, is_safe, risk_score = self.scanner.scan(s)
            max_risk = max(max_risk, risk_score)
            if not is_safe:
                unsafe_count += 1
                print(f"    âš ï¸  æ®µè½ {idx+1} é¢¨éšªåˆ†æ•¸: {risk_score:.2f}")
        
        if unsafe_count >= 2:
            print(f"  ğŸš¨ æª¢æ¸¬åˆ° {unsafe_count} å€‹é«˜é¢¨éšªæ®µè½")
            return False, max_risk
        
        print(f"  âœ… å®‰å…¨æª¢æŸ¥é€šé (æœ€é«˜é¢¨éšª: {max_risk:.2f})")
        return True, max_risk

# --- 5. æ”¹é€²çš„è§£æå™¨å·¥å»  ---
def get_converters():
    """é…ç½®å…©ç¨®è½‰æ›å™¨"""
    pdf_opts = PdfPipelineOptions()
    pdf_opts.do_ocr = True
    pdf_opts.do_table_structure = False
    pdf_opts.ocr_options = RapidOcrOptions() 

    standard_conv = DocumentConverter(
        format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=pdf_opts)}
    )
    
    vlm_opts = ApiVlmOptions(
        url=VLLM_URL,
        params=dict(
            model="allenai/olmOCR-2-7B-1025-FP8", 
            max_tokens=1500,
            temperature=0.1
        ),
        prompt="Extract all text from this image. Be concise.",
        response_format=ResponseFormat.MARKDOWN,
    )
    vlm_pipe_opts = VlmPipelineOptions(
        enable_remote_services=True, 
        vlm_options=vlm_opts
    )
    
    vlm_conv = DocumentConverter(
        format_options={
            InputFormat.PDF: PdfFormatOption(
                pipeline_options=vlm_pipe_opts, 
                pipeline_cls=VlmPipeline
            ),
            InputFormat.IMAGE: ImageFormatOption(
                pipeline_options=vlm_pipe_opts, 
                pipeline_cls=VlmPipeline
            )
        }
    )
    
    return standard_conv, vlm_conv

# --- 6. æ™ºèƒ½ä¸Šä¸‹æ–‡é¸æ“‡å™¨ ---
def select_best_chunks(candidate_objects, max_chars=MAX_CONTEXT_CHARS):
    """
    candidate_objects: list of dicts {text, score, source, orig_idx}
    """
    # 1. å…ˆæŒ‰åˆ†æ•¸å¾é«˜åˆ°ä½æ’åºï¼Œé¸å–æœ€ç›¸é—œçš„
    sorted_items = sorted(candidate_objects, key=lambda x: x["score"], reverse=True)
    
    selected = []
    total_chars = 0
    for item in sorted_items:
        if total_chars + len(item["text"]) <= max_chars:
            selected.append(item)
            total_chars += len(item["text"])
        else:
            # å¦‚æœé€™ä¸€å€‹å¤ªé•·ï¼Œå¯ä»¥è·³éæ‰¾ä¸‹ä¸€å€‹çŸ­ä¸€é»çš„ï¼Œæˆ–è€…ç›´æ¥ break
            continue
            
    # 2. æ¢å¾©åŸå§‹é †åº (é‡è¦ï¼šç¶­æŒæ–‡æª”é–±è®€çš„é‚è¼¯é †åº)
    selected.sort(key=lambda x: x["orig_idx"])
    
    return [s["text"] for s in selected], [s["source"] for s in selected], total_chars

# --- 7. DeepEval Judge (ç°¡åŒ–ç‰ˆ) ---
class SimpleJudge(DeepEvalBaseLLM):    
    def __init__(self, base_url=CHAT_API_URL, model_name=JUDGE_MODEL_ID):
        self.base_url = base_url
        self.model_name = model_name
    
    def load_model(self): 
        return OpenAI(api_key="NoNeed", base_url=self.base_url)
    
    def generate(self, prompt: str) -> str:
        client = self.load_model()
        
        # åš´æ ¼é™åˆ¶ prompt é•·åº¦
        if len(prompt) > 7000:  # å­—å…ƒé™åˆ¶
            print(f"  âš ï¸  Prompt éé•· ({len(prompt)} å­—å…ƒ)ï¼Œæˆªæ–·åˆ° 7000")
            prompt = prompt[:7000] + "\n\n[å…§å®¹å·²æˆªæ–·]"
        
        try:
            response = client.chat.completions.create(
                model=self.model_name, 
                messages=[{"role": "user", "content": prompt}], 
                temperature=0.0,  # é™ä½æº«åº¦ç¢ºä¿ JSON è¼¸å‡º
                max_tokens=512
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"  âš ï¸  LLM èª¿ç”¨å¤±æ•—: {str(e)[:100]}")
            print("prompt:",prompt)
            return "ç”Ÿæˆå¤±æ•—"
    
    async def a_generate(self, p): 
        return self.generate(p)
    
    def get_model_name(self): 
        return "Qwen3-Judge"

# --- 8. æ–‡æª”è™•ç†å‡½æ•¸ ---
def process_document_with_fallback(filepath, standard_conv, vlm_conv, guard):
    """å¸¶æœ‰å®Œæ•´éŒ¯èª¤è™•ç†çš„æ–‡æª”è™•ç†"""
    filename = os.path.basename(filepath)
    file_ext = os.path.splitext(filename)[1].lower()
    
    try:
        if file_ext in ['.png', '.jpg', '.jpeg', '.bmp', '.gif']:
            processed_path = resize_image_if_needed(filepath)
            filepath = processed_path
        
        if filename in ["1.pdf", "2.pdf"]:
            print(f"[*] ğŸš€ ä½¿ç”¨ RapidOCR è§£æ: {filename}")
            converter = standard_conv
            use_vlm = False
        else:
            print(f"[*] ğŸ§  ä½¿ç”¨ olmOCR è§£æ: {filename}")
            converter = vlm_conv
            use_vlm = True
        
        result = converter.convert(filepath)
        content = result.document.export_to_markdown()
        
        if not content or len(content.strip()) < 10:
            print(f"  âš ï¸  {filename} å…§å®¹ç‚ºç©ºï¼Œå˜—è©¦å‚™ç”¨æ–¹æ¡ˆ")
            if use_vlm:
                print(f"  ğŸ”„ åˆ‡æ›åˆ° RapidOCR")
                result = standard_conv.convert(filepath)
                content = result.document.export_to_markdown()
            else:
                print(f"  ğŸ”„ åˆ‡æ›åˆ° olmOCR")
                result = vlm_conv.convert(filepath)
                content = result.document.export_to_markdown()
        
        is_safe, risk = guard.scan_content(content, filename)
        
        if is_safe:
            return content, filename, True
        else:
            print(f"  ğŸš¨ {filename} é¢¨éšªåˆ†æ•¸éé«˜")
            return None, filename, False
            
    except Exception as e:
        print(f"  âŒ {filename} è§£æå¤±æ•—: {e}")
        return None, filename, False
    finally:
        if file_ext in ['.png', '.jpg', '.jpeg', '.bmp', '.gif']:
            temp_path = f"temp_resized_{filename}"
            if os.path.exists(temp_path):
                try:
                    os.remove(temp_path)
                except:
                    pass

# --- 9. ä¸»ç¨‹å¼ ---
def main():
    print(">>> è¼‰å…¥ Reranker æ¨¡å‹...")
    reranker_tokenizer = AutoTokenizer.from_pretrained(RERANKER_PATH, local_files_only=True)
    reranker_model = AutoModelForCausalLM.from_pretrained(
        RERANKER_PATH, local_files_only=True, torch_dtype=torch.float16
    ).to(DEVICE).eval()
    
    token_no = reranker_tokenizer.convert_tokens_to_ids("no")
    token_yes = reranker_tokenizer.convert_tokens_to_ids("yes")
    
    q_client = QdrantClient(host="localhost", port=6333)
    guard = FlexiblePDFScanner()
    custom_llm = SimpleJudge()
    chunker = SemanticChunker(max_chunk_size=MAX_CHUNK_SIZE)
    
    standard_conv, vlm_conv = get_converters()

    # --- ç¬¬ä¸€éšæ®µ: æ–‡ä»¶è§£æèˆ‡åˆ†å¡Š ---
    print("\n\033[94m>>> [ç¬¬ä¸€éšæ®µ: æ–‡ä»¶è§£æèˆ‡èªæ„åˆ†å¡Š]\033[0m")
    target_files = ["1.pdf","2.pdf","3.pdf","4.png","5.docx"]
    all_chunks = []
    all_metas = []
    
    success_count = 0
    fail_count = 0

    for f in target_files:
        if not os.path.exists(f): 
            print(f"  âš ï¸  æª”æ¡ˆä¸å­˜åœ¨: {f}")
            continue
        
        content, filename, success = process_document_with_fallback(
            f, standard_conv, vlm_conv, guard
        )
        
        if success and content:
            # ä½¿ç”¨èªæ„åˆ†å¡Š
            chunks = chunker.chunk_document(content)
            
            # ç‚ºæ¯å€‹å¡Šæ·»åŠ ä¾†æºä¿¡æ¯
            for chunk in chunks:
                all_chunks.append(chunk)
                all_metas.append(filename)
            
            success_count += 1
            print(f"  âœ… {filename} è™•ç†æˆåŠŸ â†’ {len(chunks)} å€‹å¡Š")
        else:
            fail_count += 1

    print(f"\nğŸ“Š è™•ç†çµ±è¨ˆ: æˆåŠŸ {success_count} å€‹æ–‡æª”, å¤±æ•— {fail_count} å€‹")
    print(f"ğŸ“¦ ç¸½å¡Šæ•¸: {len(all_chunks)} å€‹èªæ„å¡Š")
    
    # --- ç¬¬äºŒéšæ®µ: Qdrant å…¥åº« ---
    if not all_chunks: 
        print("âŒ æ²’æœ‰ä»»ä½•å…§å®¹ï¼Œç¨‹å¼çµæŸ")
        return
    
    def get_embs(texts):
        response = requests.post(
            EMBED_URL, 
            json={"texts": texts, "task_description": "æª¢ç´¢", "normalize": True}
        )
        return response.json()["embeddings"]
    
    dim = len(get_embs(["test"])[0])
    print(f"\n[*] å‘é‡ç¶­åº¦: {dim}")
    
    if q_client.collection_exists(COLLECTION_NAME): 
        q_client.delete_collection(COLLECTION_NAME)
    
    q_client.create_collection(
        COLLECTION_NAME, 
        vectors_config={"dense": models.VectorParams(size=dim, distance=models.Distance.COSINE)}, 
        sparse_vectors_config={"sparse": models.SparseVectorParams(modifier=models.Modifier.IDF)}
    )
    
    print(f"[*] æ­£åœ¨ç”¢ç”Ÿå‘é‡ä¸¦å…¥åº«...")
    
    # æ‰¹æ¬¡ç”Ÿæˆå‘é‡
    all_embs = []
    for i in range(0, len(all_chunks), BATCH_SIZE):
        batch = all_chunks[i : i + BATCH_SIZE]
        all_embs.extend(get_embs(batch))
    
    # å»ºç«‹ç´¢å¼•é»
    points = [
        models.PointStruct(
            id=str(uuid.uuid4()), 
            vector={
                "dense": emb, 
                "sparse": models.Document(text=chunk, model="Qdrant/bm25")
            }, 
            payload={"text": chunk, "source": source}
        ) 
        for chunk, emb, source in zip(all_chunks, all_embs, all_metas)
    ]
    
    q_client.upsert(COLLECTION_NAME, points, wait=True)
    print(f"âœ… å…¥åº«å®Œæˆ")

    # --- ç¬¬ä¸‰éšæ®µ: RAG åŸ·è¡Œèˆ‡è©•æ¸¬ ---
    print("\n\033[94m>>> [ç¬¬äºŒéšæ®µ: RAG åŸ·è¡Œ]\033[0m")
    
    if not os.path.exists('questions.csv'):
        print("âŒ questions.csv ä¸å­˜åœ¨")
        return
    
    q_df = pd.read_csv('questions.csv').head(5)
    ans_df = pd.read_csv('questions_answer.csv')
    
    # ç°¡åŒ–è©•æ¸¬æŒ‡æ¨™ï¼ˆæ¸›å°‘ API èª¿ç”¨ï¼‰
    metrics = {
        "Relevancy": AnswerRelevancyMetric(model=custom_llm,include_reason=True),
        "Faith": FaithfulnessMetric(model=custom_llm,include_reason=True),
        "Precision":ContextualPrecisionMetric(model=custom_llm,include_reason=True),
        "Recall":ContextualRecallMetric(model=custom_llm,include_reason=True)
    }
    
    final_output = []
    
    for idx, row in q_df.iterrows():
        try:
            qid = str(row['id'])
            qtxt = str(row['questions'])
            
            g_truth_rows = ans_df[ans_df['id'].astype(str) == qid]
            if len(g_truth_rows) == 0:
                print(f"âš ï¸  ID {qid} æ²’æœ‰å°æ‡‰ç­”æ¡ˆ")
                continue
            g_truth = str(g_truth_rows['answer'].values[0])

            q_emb = get_embs([qtxt])[0]
            
            # æ··åˆæª¢ç´¢ - å¢åŠ æª¢ç´¢æ•¸é‡
            search_res = q_client.query_points(
                collection_name=COLLECTION_NAME,
                prefetch=[
                    models.Prefetch(
                        query=models.Document(text=qtxt, model="Qdrant/bm25"), 
                        using="sparse", 
                        limit=20
                    ),
                    models.Prefetch(query=q_emb, using="dense", limit=20),
                ],
                query=models.FusionQuery(fusion=models.Fusion.RRF),
                limit=15  # ç²å–æ›´å¤šå€™é¸
            )

            candidates = [p.payload["text"] for p in search_res.points]
            if not candidates: 
                print(f"âš ï¸  ID {qid} æ²’æœ‰æª¢ç´¢çµæœ")
                continue
            # ... å‰é¢æª¢ç´¢éƒ¨åˆ†ä¸è®Š ...
            candidates_text = [p.payload["text"] for p in search_res.points]
            candidates_source = [p.payload["source"] for p in search_res.points]
            
            if not candidates_text: 
                print(f"âš ï¸  ID {qid} æ²’æœ‰æª¢ç´¢çµæœ")
                continue
            
            # Rerank
            rerank_pairs = [[qtxt, c] for c in candidates_text]
            inputs = reranker_tokenizer(
                rerank_pairs, 
                padding=True, 
                truncation=True, 
                return_tensors='pt',
                max_length=512
            ).to(DEVICE)
            
            with torch.no_grad():
                logits = reranker_model(**inputs).logits[:, -1, [token_no, token_yes]]
                scores = torch.softmax(logits, dim=-1)[:, 1].tolist()
            
            # --- é—œéµä¿®æ­£ï¼šå°è£ç‰©ä»¶ä»¥ä¾¿ select_best_chunks è™•ç† ---
            candidate_objs = []
            for i in range(len(candidates_text)):
                candidate_objs.append({
                    "text": candidates_text[i],
                    "score": scores[i],
                    "source": candidates_source[i],
                    "orig_idx": i  # é€™è£¡ä¿ç•™æª¢ç´¢å‡ºä¾†çš„åŸå§‹é †åº
                })
            
            # æ™ºèƒ½é¸æ“‡æœ€ä½³å¡Š
            best_chunks, best_sources_list, total_chars = select_best_chunks(candidate_objs, MAX_CONTEXT_CHARS)
            best_sources = list(set(best_sources_list))
            
            print(f"\n>> ID {qid}:")
            print(f"  ğŸ“Š é¸ä¸­ {len(best_chunks)} å€‹å¡Šï¼Œå…± {total_chars} å­—å…ƒ")
            
            # ç”Ÿæˆç­”æ¡ˆ
            context_text = "\n\n".join(best_chunks)
            prompt = f"è«‹æ ¹æ“šä»¥ä¸‹è³‡æ–™å›ç­”å•é¡Œã€‚\n\nè³‡æ–™ï¼š\n{context_text}\n\nå•é¡Œï¼š{qtxt}\n\nç­”æ¡ˆï¼š"
            
            # ... å¾ŒçºŒç”Ÿæˆèˆ‡è©•æ¸¬é‚è¼¯ ...
        
            
            print(f"  ğŸ“ Prompt é•·åº¦: {len(prompt)} å­—å…ƒ")
            
            ans = custom_llm.generate(prompt)
            print("ans:", ans)
            # ç°¡åŒ–è©•æ¸¬ï¼ˆåªè©•é—œéµæŒ‡æ¨™ï¼‰
            print(f"  è©•æ¸¬çµæœ:")
            tc = LLMTestCase(
                input=qtxt, 
                actual_output=ans, 
                retrieval_context=best_chunks,
                expected_output=g_truth
            )
            
            # å»ºç«‹é€™ä¸€åˆ—çš„çµæœç´€éŒ„
            row_result = {
                'q_id': qid, 
                'questions': qtxt, 
                'answer': ans, 
                'source': ",".join(best_sources)
            }

            print(f"  è©•æ¸¬çµæœ:")
            for name, m in metrics.items():
                try:
                    m.measure(tc)
                    print(f"   [*] {name}: {m.score:.2f}")
                    # å°‡åˆ†æ•¸èˆ‡åŸå› å­˜å…¥è©²åˆ—å­—å…¸
                    row_result[f'{name}_score'] = m.score
                    row_result[f'{name}_reason'] = getattr(m, 'reason', 'N/A')
                except Exception as e:
                    print(f"   [!] {name} è©•æ¸¬å¤±æ•—: {e}")
                    row_result[f'{name}_score'] = 0
                    row_result[f'{name}_reason'] = "Error"

            final_output.append(row_result)
            
        except Exception as e: 
            print(f"âŒ ID {qid} å¤±æ•—: {e}")

    # å„²å­˜çµæœ
    # if final_output:
    #     pd.DataFrame(final_output).to_csv(
    #         'test_dataset_final.csv', 
    #         index=False, 
    #         encoding='utf-8-sig'
    #     )
    #     print(f"\nâœ… å®Œæˆï¼å…±ç”Ÿæˆ {len(final_output)} å€‹ç­”æ¡ˆ")
    # else:
    #     print("\nâš ï¸  æ²’æœ‰ç”Ÿæˆç­”æ¡ˆ")
    # --- ç¬¬å››éšæ®µ: ç¨ç«‹è™•ç† test_dataset.csv (ç„¡è©•æ¸¬æ¨¡å¼) ---
    if os.path.exists('test_dataset.csv'):
        print("\n\033[92m>>> [ç¬¬å››éšæ®µ: è™•ç† test_dataset.csv]\033[0m")
        test_df = pd.read_csv('test_dataset.csv')
        test_final_output = []

        for _, row in test_df.iterrows():
            try:
                tid = str(row.get('id', 'N/A'))
                tqtxt = str(row.get('questions', ''))
                if not tqtxt: continue

                print(f"[*] æ­£åœ¨è™•ç†æ¸¬è©¦é›† ID: {tid}...", end="\r")

                # 1. æª¢ç´¢
                t_q_emb = get_embs([tqtxt])[0]
                t_search_res = q_client.query_points(
                    collection_name=COLLECTION_NAME,
                    prefetch=[
                        models.Prefetch(query=models.Document(text=tqtxt, model="Qdrant/bm25"), using="sparse", limit=20),
                        models.Prefetch(query=t_q_emb, using="dense", limit=20),
                    ],
                    query=models.FusionQuery(fusion=models.Fusion.RRF),
                    limit=15
                )

                t_cand_text = [p.payload["text"] for p in t_search_res.points]
                t_cand_src = [p.payload["source"] for p in t_search_res.points]

                if not t_cand_text:
                    test_final_output.append({'id': tid, 'questions': tqtxt, 'answer': "æŸ¥ç„¡è³‡æ–™", 'source': ""})
                    continue

                # 2. Rerank
                t_rerank_pairs = [[tqtxt, c] for c in t_cand_text]
                t_inputs = reranker_tokenizer(t_rerank_pairs, padding=True, truncation=True, return_tensors='pt', max_length=512).to(DEVICE)
                with torch.no_grad():
                    t_logits = reranker_model(**t_inputs).logits[:, -1, [token_no, token_yes]]
                    t_scores = torch.softmax(t_logits, dim=-1)[:, 1].tolist()

                # 3. æ™ºèƒ½é¸æ“‡
                t_objs = []
                for i in range(len(t_cand_text)):
                    t_objs.append({
                        "text": t_cand_text[i],
                        "score": t_scores[i],
                        "source": t_cand_src[i],
                        "orig_idx": i
                    })
                
                t_best_chunks, t_best_srcs, _ = select_best_chunks(t_objs, MAX_CONTEXT_CHARS)

                # 4. ç”Ÿæˆç­”æ¡ˆ
                t_context = "\n\n".join(t_best_chunks)
                t_prompt = f"è«‹æ ¹æ“šä»¥ä¸‹è³‡æ–™å›ç­”å•é¡Œã€‚\n\nè³‡æ–™ï¼š\n{t_context}\n\nå•é¡Œï¼š{tqtxt}\n\nç­”æ¡ˆï¼š"
                t_ans = custom_llm.generate(t_prompt)

                test_final_output.append({
                    'id': tid,
                    'questions': tqtxt,
                    'answer': t_ans,
                    'source': ",".join(list(set(t_best_srcs)))
                })

            except Exception as e:
                print(f"\nâŒ æ¸¬è©¦é›† ID {tid} å¤±æ•—: {e}")

        # å„²å­˜æ¸¬è©¦é›†çµæœ
        if test_final_output:
            pd.DataFrame(test_final_output).to_csv('test_dataset_final.csv', index=False, encoding='utf-8-sig')
            print(f"\nâœ… æ¸¬è©¦é›†è™•ç†å®Œæˆï¼Œå·²å­˜è‡³ test_dataset_final.csv")
    else:
        print("\nâ„¹ï¸ æœªç™¼ç¾ test_dataset.csvï¼Œè·³éæ­¤éšæ®µã€‚")

if __name__ == "__main__":
    main()