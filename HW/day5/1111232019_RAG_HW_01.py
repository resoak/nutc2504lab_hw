import os
import uuid
import pandas as pd
import requests
import time
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

# === ä¿®æ­£å¾Œçš„ Import ===
from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain_experimental.text_splitter import SemanticChunker

# === 0. é…ç½®èˆ‡åˆå§‹åŒ– ===
API_KEY = "YOUR_API_KEY" 
EMBED_API_URL = "https://ws-04.wade0426.me/embed"
SUBMIT_URL = "https://hw-01.wade0426.me/submit_answer"
CHUNK_SIZE = 300
CHUNK_OVERLAP = 50

client = QdrantClient(url="http://localhost:6333")

class CustomEmbeddings:
    def embed_documents(self, texts): return get_embeddings(texts)
    def embed_query(self, text): return get_embeddings([text])[0]

# === 1. åŠŸèƒ½å‡½æ•¸ ===

def get_embeddings(texts):
    if not texts: return []
    payload = {"texts": texts, "normalize": True, "batch_size": 32}
    try:
        response = requests.post(EMBED_API_URL, json=payload, timeout=60)
        response.raise_for_status()
        return response.json()['embeddings']
    except Exception as e:
        print(f"âŒ Embedding API éŒ¯èª¤: {e}")
        return []

def submit_and_get_score(q_id, answer):
    payload = {"q_id": q_id, "student_answer": answer}
    try:
        response = requests.post(SUBMIT_URL, json=payload, timeout=20)
        return response.json().get("score", 0) if response.status_code == 200 else 0
    except:
        return 0

# === 2. æª”æ¡ˆè™•ç†èˆ‡åˆ‡å¡Š (åŠ å…¥ Metadata) ===

def process_files_and_chunk():
    data_files = [f"data_0{i}.txt" for i in range(1, 6)]
    all_chunks_data = {"å›ºå®šå¤§å°": [], "æ»‘å‹•è¦–çª—": [], "èªç¾©åˆ‡å¡Š": []}
    embeddings_tool = CustomEmbeddings()
    
    semantic_sub_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=0)
    
    print("\n" + "="*20 + " 1. é–‹å§‹æª”æ¡ˆåˆ‡å¡Šéšæ®µ " + "="*20)
    for file_name in data_files:
        if not os.path.exists(file_name):
            continue
        with open(file_name, "r", encoding="utf-8") as f:
            content = f.read()
        
        print(f"ğŸ“„ è®€å–æª”æ¡ˆ: {file_name} ({len(content)} å­—)")
        
        # 1. å›ºå®šå¤§å°
        f_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=0, separator="")
        for c in [d.page_content for d in f_splitter.create_documents([content])]:
            all_chunks_data["å›ºå®šå¤§å°"].append({"text": c, "source": file_name})
        
        # 2. æ»‘å‹•è¦–çª—
        s_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
        for c in [d.page_content for d in s_splitter.create_documents([content])]:
            all_chunks_data["æ»‘å‹•è¦–çª—"].append({"text": c, "source": file_name})
        
        # 3. èªç¾©åˆ‡å¡Š
        sem_splitter = SemanticChunker(
            embeddings_tool, 
            breakpoint_threshold_type="percentile",
            breakpoint_threshold_amount=95
        )
        sem_base_docs = sem_splitter.create_documents([content])
        
        for doc in sem_base_docs:
            if len(doc.page_content) > CHUNK_SIZE:
                sub_docs = semantic_sub_splitter.split_text(doc.page_content)
                for sub_c in sub_docs:
                    all_chunks_data["èªç¾©åˆ‡å¡Š"].append({"text": sub_c, "source": file_name})
            else:
                all_chunks_data["èªç¾©åˆ‡å¡Š"].append({"text": doc.page_content, "source": file_name})
        
    return all_chunks_data

# === 3. å‘é‡æª¢ç´¢èˆ‡è©•åˆ† (ä¿®æ”¹å¾Œï¼šæª¢æŸ¥è³‡æ–™åº«æ˜¯å¦å­˜åœ¨) ===

def setup_vdb_and_search():
    results_for_csv = []
    
    # è®€å–å•é¡Œ
    questions_df = pd.read_csv("questions.csv")
    q_texts = questions_df['questions'].astype(str).tolist()
    q_ids = questions_df['q_id'].tolist()
    
    method_to_coll = {
        "å›ºå®šå¤§å°": "coll_fixed_size",
        "æ»‘å‹•è¦–çª—": "coll_sliding_window",
        "èªç¾©åˆ‡å¡Š": "coll_semantic_chunk"
    }
    
    print(f"\nğŸ“¡ æ­£åœ¨æ‰¹é‡ç²å– {len(q_texts)} å€‹å•é¡Œçš„å‘é‡...")
    all_q_vectors = get_embeddings(q_texts)
    
    # å»¶é²åˆ‡å¡Šï¼šåªæœ‰åœ¨å¿…è¦æ™‚æ‰å‘¼å«åˆ‡å¡Šå‡½æ•¸
    all_chunks_data = None 

    print("\n" + "="*20 + " 2. å‘é‡æª¢ç´¢èˆ‡è©•åˆ†éšæ®µ " + "="*20)

    for method, coll_name in method_to_coll.items():
        print(f"\nğŸ› ï¸ æ­£åœ¨ç¢ºèªæ–¹æ³•: [{method}]")
        
        # --- ä¿®æ”¹è™•ï¼šæª¢æŸ¥ Collection æ˜¯å¦å­˜åœ¨ ---
        if not client.collection_exists(collection_name=coll_name):
            print(f"â„¹ï¸ {coll_name} ä¸å­˜åœ¨ï¼Œé–‹å§‹åˆå§‹åŒ–è³‡æ–™...")
            
            # åªæœ‰ç¬¬ä¸€æ¬¡éœ€è¦æ™‚æ‰åŸ·è¡Œåˆ‡å¡Š
            if all_chunks_data is None:
                all_chunks_data = process_files_and_chunk()
            
            chunk_items = all_chunks_data[method]
            texts = [item['text'] for item in chunk_items]
            sources = [item['source'] for item in chunk_items]
            
            chunk_vectors = get_embeddings(texts)
            if not chunk_vectors:
                print(f"âš ï¸ ç„¡æ³•å–å¾—å‘é‡ï¼Œè·³éæ–¹æ³•: {method}")
                continue

            # å»ºç«‹ Collection
            client.create_collection(
                collection_name=coll_name,
                vectors_config=VectorParams(size=len(chunk_vectors[0]), distance=Distance.COSINE)
            )
            
            # å­˜å…¥è³‡æ–™
            points = [
                PointStruct(
                    id=uuid.uuid4().hex, 
                    vector=chunk_vectors[i], 
                    payload={"text": texts[i], "source": sources[i]}
                ) for i in range(len(texts))
            ]
            client.upsert(collection_name=coll_name, points=points)
            print(f"âœ… {coll_name} è³‡æ–™åˆå§‹åŒ–å®Œæˆã€‚")
        else:
            print(f"âœ… {coll_name} å·²å­˜åœ¨ï¼Œè·³éä¸Šå‚³æ­¥é©Ÿã€‚")

        # --- åŸ·è¡Œæª¢ç´¢èˆ‡è©•åˆ† (ç„¡è«–æ˜¯å¦ç‚ºæ–°å»ºç«‹éƒ½æœƒåŸ·è¡Œ) ---
        for i, q_vec in enumerate(all_q_vectors):
            search_res = client.query_points(
                collection_name=coll_name, 
                query=q_vec, 
                limit=3
            ).points
            
            retrieved_content = "\n".join([h.payload['text'] for h in search_res])
            unique_sources = list(set([h.payload['source'] for h in search_res]))
            source_str = ",".join(unique_sources)
            
            score = submit_and_get_score(q_ids[i], retrieved_content)
            
            if i % 20 == 0:
                print(f"   ğŸ“ Q{q_ids[i]} | Score: {score:.4f} | Method: {method}")
            
            results_for_csv.append({
                "q_id": q_ids[i],
                "method": method,
                "retrieve_text": retrieved_content,
                "score": score,
                "source": source_str
            })
            
    return results_for_csv

# === 4. ä¸»ç¨‹å¼ ===

if __name__ == "__main__":
    start_time = time.time()
    
    # ä¿®æ”¹ï¼šç›´æ¥é€²å…¥è³‡æ–™åº«è™•ç†æµç¨‹ï¼Œå…§å«åˆ‡å¡Šé‚è¼¯
    final_results = setup_vdb_and_search()
    
    # è¼¸å‡º CSV
    df_output = pd.DataFrame(final_results)
    df_output.insert(0, 'id', [uuid.uuid4().hex[:8] for _ in range(len(df_output))])
    
    output_name = "1111232019_RAG_HW_01.csv"
    df_output.to_csv(output_name, index=False, encoding="utf-8-sig")
    
    print("\n" + "="*30 + " 3. åŸ·è¡Œçµ±è¨ˆ " + "="*30)
    if not df_output.empty:
        avg_scores = df_output.groupby('method')['score'].mean()
        for m, s in avg_scores.items():
            print(f"   ğŸ”¹ {m} å¹³å‡åˆ†: {s:.4f}")
    
    print(f"\nâœ… å…¨éƒ¨å®Œæˆï¼ç¸½è€—æ™‚: {time.time() - start_time:.2f} ç§’")
    print(f"âœ… çµæœå·²å„²å­˜è‡³: {output_name}")