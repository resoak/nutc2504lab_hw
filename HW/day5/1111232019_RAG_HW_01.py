import os
import uuid
import pandas as pd
import requests
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain_experimental.text_splitter import SemanticChunker

# === 0. é…ç½®èˆ‡åˆå§‹åŒ– ===
API_KEY = "YOUR_API_KEY" 
EMBED_API_URL = "https://ws-04.wade0426.me/embed"
SUBMIT_URL = "https://hw-01.wade0426.me/submit_answer"

client = QdrantClient(url="http://localhost:6333")

class CustomEmbeddings:
    def embed_documents(self, texts): return get_embeddings(texts)
    def embed_query(self, text): return get_embeddings([text])[0]

# === 1. åŠŸèƒ½å‡½æ•¸ ===

def get_embeddings(texts):
    payload = {"texts": texts, "normalize": True, "batch_size": 32}
    try:
        response = requests.post(EMBED_API_URL, json=payload)
        response.raise_for_status()
        return response.json()['embeddings']
    except Exception as e:
        print(f"âŒ Embedding API éŒ¯èª¤: {e}")
        return []

def submit_and_get_score(q_id, answer):
    payload = {"q_id": q_id, "student_answer": answer}
    try:
        response = requests.post(SUBMIT_URL, json=payload)
        return response.json().get("score", 0) if response.status_code == 200 else 0
    except:
        return 0

# === 2. æª”æ¡ˆè™•ç†èˆ‡ä¸‰ç¨®åˆ‡å¡Š ===

def process_files_and_chunk():
    data_files = [f"data_0{i}.txt" for i in range(1, 6)]
    all_chunks = {"å›ºå®šå¤§å°": [], "æ»‘å‹•è¦–çª—": [], "èªç¾©åˆ‡å¡Š": []}
    chunk_source_map = {}
    embeddings_tool = CustomEmbeddings()
    
    print("\n" + "="*20 + " 1. é–‹å§‹æª”æ¡ˆåˆ‡å¡Šéšæ®µ " + "="*20)
    for file_name in data_files:
        if not os.path.exists(file_name):
            print(f"âš ï¸ è·³éä¸å­˜åœ¨çš„æª”æ¡ˆ: {file_name}")
            continue
        with open(file_name, "r", encoding="utf-8") as f:
            content = f.read()
        
        print(f"ğŸ“„ è®€å–æª”æ¡ˆ: {file_name} ({len(content)} å­—)")
        
        f_chunks = [d.page_content for d in CharacterTextSplitter(chunk_size=300, chunk_overlap=0, separator="").create_documents([content])]
        s_chunks = [d.page_content for d in RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50).create_documents([content])]
        sem_chunks = [d.page_content for d in SemanticChunker(embeddings_tool).create_documents([content])]

        for method, chunks in [("å›ºå®šå¤§å°", f_chunks), ("æ»‘å‹•è¦–çª—", s_chunks), ("èªç¾©åˆ‡å¡Š", sem_chunks)]:
            all_chunks[method].extend(chunks)
            for c in chunks: chunk_source_map[c] = file_name
        
    return all_chunks, chunk_source_map

# === 3. å‘é‡æª¢ç´¢èˆ‡è©•åˆ† (å„ªåŒ–è­¦å‘Šéƒ¨åˆ†) ===

# === 3. å‘é‡æª¢ç´¢èˆ‡è©•åˆ† (å„ªåŒ–ä¸¦æ–°å¢ Collection åç¨±é¡¯ç¤º) ===

def setup_vdb_and_search(all_methods_chunks, chunk_source_map):
    results_for_csv = []
    
    # è®€å–å•é¡Œä¸¦ä¸€æ¬¡æ€§é€²è¡Œæ‰¹é‡ Embedding
    questions_df = pd.read_csv("questions.csv")
    q_texts = questions_df['questions'].astype(str).tolist()
    q_ids = questions_df['q_id'].tolist()
    
    print(f"\nğŸ“¡ æ­£åœ¨æ‰¹é‡ç²å– {len(q_texts)} å€‹å•é¡Œçš„å‘é‡...")
    all_q_vectors = get_embeddings(q_texts)
    
    print("\n" + "="*20 + " 2. é–‹å§‹æ‰¹é‡å‘é‡æª¢ç´¢èˆ‡è©•åˆ† " + "="*20)

    for method, chunks in all_methods_chunks.items():
        coll_name = f"hw_{uuid.uuid4().hex[:8]}"
        print(f"\nğŸ› ï¸ è™•ç†æ–¹æ³•: [{method}] | Collection: {coll_name}")
        
        # ğŸš€ æ‰¹é‡ 1: ä¸€æ¬¡æ€§ç²å–æ‰€æœ‰ Chunks çš„å‘é‡
        print(f"   â¬†ï¸ æ­£åœ¨ä¸Šå‚³ {len(chunks)} å€‹æ–‡æœ¬å€å¡Š...")
        chunk_vectors = get_embeddings(chunks)
        
        if client.collection_exists(coll_name):
            client.delete_collection(coll_name)
        
        client.create_collection(
            collection_name=coll_name,
            vectors_config=VectorParams(size=len(chunk_vectors[0]), distance=Distance.COSINE)
        )
        
        points = [
            PointStruct(id=i, vector=chunk_vectors[i], payload={"text": chunks[i]}) 
            for i in range(len(chunks))
        ]
        client.upsert(collection_name=coll_name, points=points)

        # ğŸš€ æ‰¹é‡ 2: æª¢ç´¢èˆ‡è©•åˆ†å„ªåŒ–
        # é›–ç„¶è©•åˆ† API é€šå¸¸æ˜¯å–®é»æäº¤ï¼Œä½†æˆ‘å€‘å¯ä»¥å„ªåŒ–æª¢ç´¢é‚è¼¯
        for i, q_vec in enumerate(all_q_vectors):
            # é€™è£¡å¯ä»¥ä½¿ç”¨ Qdrant çš„ batch æœå°‹ APIï¼Œä½†ç‚ºäº†ç¶­æŒ logic æ¸…æ™°ï¼Œæˆ‘å€‘æ‰¹é‡è™•ç†è®Šæ•¸
            search_res = client.query_points(
                collection_name=coll_name, 
                query=q_vec, 
                limit=1
            ).points
            
            retrieved_text = search_res[0].payload['text'] if search_res else ""
            
            # æäº¤è©•åˆ† (æ­¤è™•è‹¥ API æ”¯æ´ Batch æäº¤æœƒæ›´å¿«)
            score = submit_and_get_score(q_ids[i], retrieved_text)
            
            if i % 5 == 0: # æ¸›å°‘ log åˆ·å±ï¼Œæ¯ 5 é¡Œå°ä¸€æ¬¡
                print(f"   ğŸ“ å·²è™•ç† Q{q_ids[i]} | Score: {score:.4f}")
            
            results_for_csv.append({
                "q_id": q_ids[i],
                "method": method,
                "retrieve_text": retrieved_text,
                "score": score,
                "source": chunk_source_map.get(retrieved_text, "unknown")
            })
        
        # é¸é …ï¼šæ¸…ç† Collection ç¯€çœè¨˜æ†¶é«”
        # client.delete_collection(coll_name)
            
    return results_for_csv

# === 4. ä¸»ç¨‹å¼ ===

if __name__ == "__main__":
    all_chunks, source_map = process_files_and_chunk()
    final_results = setup_vdb_and_search(all_chunks, source_map)
    
    df_output = pd.DataFrame(final_results)
    df_output.insert(0, 'id', [uuid.uuid4().hex[:8] for _ in range(len(df_output))])
    
    output_name = "1111232019_RAG_HW_01.csv"
    df_output.to_csv(output_name, index=False, encoding="utf-8-sig")
    
    print("\n" + "="*30 + " 3. æœ€çµ‚ CSV åŸ·è¡Œçµæœ (60 ç­†) " + "="*30)
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', 1000)
    pd.set_option('display.max_rows', 60)
    # æ‰“å°å‰ 60 ç­†çš„é‡è¦æ¬„ä½ä¾›å¿«é€Ÿæª¢æŸ¥
    print(df_output[['id', 'q_id', 'method', 'score', 'source']])
    
    print("\n" + "="*60)
    avg_scores = df_output.groupby('method')['score'].mean()
    print("ğŸ’¡ å„åˆ‡å¡Šæ–¹æ³•å¹³å‡åˆ†æ•¸çµ±è¨ˆï¼š")
    for m, s in avg_scores.items():
        print(f"   ğŸ”¹ {m}: {s:.4f}")
    print("="*60)