import os
import glob
import pandas as pd
import uuid
import time
from typing import List, Dict

# LangChain ç›¸é—œçµ„ä»¶
from langchain_openai import ChatOpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter
import requests

# Qdrant ç›¸é—œçµ„ä»¶
from qdrant_client import QdrantClient, models

# === 1. é…ç½®èˆ‡åˆå§‹åŒ– ===
VLM_BASE_URL = "https://ws-05.huannago.com/v1"
VLM_MODEL = "google/gemma-3-27b-it"
EMBED_URL = "https://ws-04.wade0426.me/embed"
COLLECTION_NAME = "gemma_multi_turn_rag"

# åˆå§‹åŒ– LLM (Gemma-3-27b-it)
llm = ChatOpenAI(
    base_url=VLM_BASE_URL,
    api_key="YOUR_API_KEY", # âš ï¸ è«‹åœ¨æ­¤è™•å¡«å…¥æ‚¨çš„ API Key
    model=VLM_MODEL,
    temperature=0,
    timeout=120
)

# é€£ç·šè‡³æœ¬åœ° Qdrant (Dashboard: http://localhost:6333)
client = QdrantClient(url="http://localhost:6333")

# === 2. å‘é‡åŒ–å·¥å…·å‡½æ•¸ ===
def get_embeddings(texts: List[str]) -> List[List[float]]:
    payload = {"texts": texts, "normalize": True, "task_description": "æª¢ç´¢æŠ€è¡“èˆ‡ç”Ÿæ´»æ–‡ä»¶"}
    try:
        response = requests.post(EMBED_URL, json=payload, timeout=60)
        return response.json()["embeddings"]
    except Exception as e:
        print(f"âŒ Embedding å¤±æ•—: {e}")
        return []

# === 3. åˆå§‹åŒ–çŸ¥è­˜åº« ===
def initialize_db():
    print("\n" + "="*50)
    print("ğŸ“¡ [æ­¥é©Ÿ 1/2] æ­£åœ¨åˆå§‹åŒ–æœ¬åœ° Qdrant çŸ¥è­˜åº«...")
    print("="*50)
    
    sample_vec = get_embeddings(["check"])[0]
    dim = len(sample_vec)
    
    if client.collection_exists(COLLECTION_NAME):
        print(f"ğŸ—‘ï¸  åµæ¸¬åˆ°èˆŠé›†åˆï¼Œæ­£åœ¨åˆªé™¤ä¸¦é‡å»º: {COLLECTION_NAME}")
        client.delete_collection(COLLECTION_NAME)
    
    client.create_collection(
        collection_name=COLLECTION_NAME,
        vectors_config=models.VectorParams(size=dim, distance=models.Distance.COSINE)
    )
    
    file_paths = glob.glob("data_0*.txt")
    splitter = RecursiveCharacterTextSplitter(chunk_size=350, chunk_overlap=50)
    all_points = []
    
    for path in file_paths:
        file_name = os.path.basename(path)
        print(f"ğŸ“– æ­£åœ¨è®€å–ä¸¦åˆ‡åˆ†æª”æ¡ˆ: {file_name}")
        with open(path, 'r', encoding='utf-8') as f:
            chunks = splitter.split_text(f.read())
            vectors = get_embeddings(chunks)
            for chunk, vec in zip(chunks, vectors):
                all_points.append(models.PointStruct(
                    id=str(uuid.uuid4()),
                    vector=vec,
                    payload={"text": chunk, "source": file_name}
                ))
    
    client.upsert(collection_name=COLLECTION_NAME, points=all_points)
    print(f"âœ… çŸ¥è­˜åº«åŒ¯å…¥å®Œæˆï¼Œå…±è¨ˆ {len(all_points)} å€‹è³‡æ–™é»ã€‚")

# === 4. åŸ·è¡Œå¤šè¼ª RAG ä»»å‹™ (å…¨éç¨‹ Print) ===
def run_rag_task():
    input_file = "Re_Write_questions.csv" 
    if not os.path.exists(input_file):
        print(f"âŒ æ‰¾ä¸åˆ°ä¾†æºæª”æ¡ˆ: {input_file}")
        return
    
    df = pd.read_csv(input_file)
    df.columns = df.columns.str.strip()
    
    if os.path.exists("Prompt_ReWrite.txt"):
        with open("Prompt_ReWrite.txt", "r", encoding="utf-8") as f:
            rewrite_instruction = f.read()
    else:
        rewrite_instruction = "ä½ æ˜¯ä¸€å€‹æŸ¥è©¢é‡å¯«å°ˆå®¶ã€‚è«‹æ ¹æ“šå°è©±æ­·å²å°‡æœ€æ–°å•é¡Œæ”¹å¯«ç‚ºç¨ç«‹çš„æœå°‹èªå¥ã€‚"

    session_history = {} 
    final_answers = []
    final_sources = []

    print("\n" + "="*50)
    print(f"ğŸš€ [æ­¥é©Ÿ 2/2] é–‹å§‹è™•ç†å•é¡Œé›†: {input_file}")
    print("="*50)

    for index, row in df.iterrows():
        cid = str(row['conversation_id'])
        original_q = str(row['questions']) 
        history_str = session_history.get(cid, "ç„¡å°è©±æ­·å²")

        print(f"\nğŸ‘‰ [ç¬¬ {index+1} é¡Œ] æœƒè©± ID: {cid}")
        print(f"   [åŸå§‹å•é¡Œ]: {original_q}")

        # Step 1: Query Rewrite
        print(f"   [æ­£åœ¨æ”¹å¯«æŸ¥è©¢ä¸­...]")
        rewrite_prompt = f"{rewrite_instruction}\n\n[å°è©±æ­·å²]:\n{history_str}\n\n[æœ€æ–°å•é¡Œ]:\n{original_q}\n\nè«‹ç›´æ¥è¼¸å‡ºé‡å¯«å¾Œçš„æœå°‹èªå¥ï¼š"
        try:
            rewritten_q = llm.invoke(rewrite_prompt).content.strip()
            print(f"   [æ”¹å¯«çµæœ]: {rewritten_q}")
        except Exception as e:
            print(f"   âš ï¸ æ”¹å¯«å¤±æ•— ({e})ï¼Œä½¿ç”¨åŸå¥æœå°‹ã€‚")
            rewritten_q = original_q

        # Step 2: Retrieval
        print(f"   [æ­£åœ¨æª¢ç´¢å‘é‡è³‡æ–™åº«...]")
        q_vec = get_embeddings([rewritten_q])[0]
        search_results = client.query_points(
            collection_name=COLLECTION_NAME,
            query=q_vec,
            limit=5
        ).points
        
        context_list = [hit.payload['text'] for hit in search_results]
        context_str = "\n".join(context_list)
        top_source = search_results[0].payload['source'] if search_results else "æœªçŸ¥ä¾†æº"
        
        print(f"   [æª¢ç´¢åˆ°ä¾†æº]: {top_source}")
        # print(f"   [åƒè€ƒç‰‡æ®µ]: {context_list[0][:50]}...") # è‹¥æƒ³çœ‹æ›´ç´°å¯è§£é–é€™è¡Œ

        # Step 3: Generation
        print(f"   [æ­£åœ¨ç”Ÿæˆæœ€çµ‚å›ç­”...]")
        final_prompt = f"""è«‹åš´æ ¼æ ¹æ“šåƒè€ƒè³‡è¨Šå›ç­”ã€‚è³‡è¨Šä¸è¶³è«‹å›ã€ŒæŠ±æ­‰ï¼Œæˆ‘ç„¡æ³•å›ç­”ã€ã€‚
ã€åƒè€ƒè³‡è¨Šã€‘ï¼š
{context_str}
ã€å•é¡Œã€‘ï¼š{rewritten_q}
å›ç­”ï¼š"""
        
        try:
            answer = llm.invoke(final_prompt).content.strip()
            print(f"   [æ©Ÿå™¨å›ç­”]: {answer[:50]}...")
        except Exception as e:
            answer = "æŠ±æ­‰ï¼Œç³»çµ±ç”Ÿæˆå›ç­”æ™‚å‡ºéŒ¯ã€‚"
            print(f"   âŒ å›ç­”ç”Ÿæˆå¤±æ•—: {e}")
        
        # æ›´æ–°æ­·å²
        session_history[cid] = history_str + f"\nå•ï¼š{original_q}\nç­”ï¼š{answer}\n"
        
        final_answers.append(answer)
        final_sources.append(top_source)
        time.sleep(0.5)

    # å„²å­˜çµæœ
    df['answer'] = final_answers
    df['source'] = final_sources
    
    output_csv = "Re_Write_questions_result.csv"
    df.to_csv(output_csv, index=False, encoding="utf-8-sig")
    print("\n" + "="*50)
    print(f"ğŸ‰ ä»»å‹™åœ“æ»¿å®Œæˆï¼")
    print(f"ğŸ’¾ çµæœæª”æ¡ˆå·²å„²å­˜è‡³: {output_csv}")
    print("="*50)

if __name__ == "__main__":
    initialize_db()
    run_rag_task()