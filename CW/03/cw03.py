import os
import glob
import pandas as pd
import uuid
import requests
import sys
import time
from typing import List

# LangChain èˆ‡æ¨¡å‹ç›¸é—œçµ„ä»¶
from langchain_openai import ChatOpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter
from qdrant_client import QdrantClient, models

# === 1. é…ç½®èˆ‡åˆå§‹åŒ– ===
VLM_BASE_URL = "https://ws-02.wade0426.me/v1"
VLM_MODEL = "google/gemma-3-27b-it"
EMBED_URL = "https://ws-04.wade0426.me/embed"
COLLECTION_NAME = "gemma_multi_turn_rag"

# è«‹ç¢ºä¿ API Key æ­£ç¢º
llm = ChatOpenAI(
    base_url=VLM_BASE_URL,
    api_key="YOUR_API_KEY", 
    model=VLM_MODEL,
    temperature=0,
    timeout=60 
)

client = QdrantClient(url="http://localhost:6333")

# === 2. é«˜é€Ÿå‘é‡åŒ–å·¥å…·å‡½æ•¸ (æ”¯æ´æ‰¹æ¬¡è™•ç†èˆ‡é‡è©¦) ===
def get_embeddings_batch(texts: List[str]) -> List[List[float]]:
    if not texts: return []
    payload = {"texts": texts, "normalize": True, "task_description": "æª¢ç´¢æŠ€è¡“èˆ‡ç”Ÿæ´»æ–‡ä»¶"}
    for attempt in range(3):
        try:
            response = requests.post(EMBED_URL, json=payload, timeout=60)
            response.raise_for_status()
            return response.json().get("embeddings", [])
        except Exception as e:
            print(f"  âš ï¸ Embedding å˜—è©¦ {attempt+1} å¤±æ•—: {e}")
            time.sleep(2)
    return []

# === 3. åˆå§‹åŒ–çŸ¥è­˜åº« (é«˜é€Ÿç‰ˆ) ===
def initialize_db():
    print("\n" + "="*50)
    print("ğŸ“¡ [æ­¥é©Ÿ 1/2] æ­£åœ¨é«˜é€Ÿåˆå§‹åŒ–çŸ¥è­˜åº«...")
    
    sample = get_embeddings_batch(["check"])
    if not sample:
        print("ğŸ›‘ å‘é‡ä¼ºæœå™¨é€£ç·šå¤±æ•—ï¼Œç¨‹å¼åœæ­¢ã€‚")
        sys.exit(1)
        
    dim = len(sample[0])
    # å¿«é€Ÿé‡ç½® Collection
    client.recreate_collection(
        collection_name=COLLECTION_NAME,
        vectors_config=models.VectorParams(size=dim, distance=models.Distance.COSINE)
    )
    
    file_paths = glob.glob("data_0*.txt")
    splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)
    
    for path in file_paths:
        file_name = os.path.basename(path)
        print(f"ğŸ“– è™•ç†æª”æ¡ˆ: {file_name}...", end="", flush=True)
        with open(path, 'r', encoding='utf-8-sig', errors='replace') as f:
            content = f.read().replace('\ufffd', '')
            chunks = splitter.split_text(content)
            vectors = get_embeddings_batch(chunks)
            if vectors:
                points = [models.PointStruct(
                    id=str(uuid.uuid4()), 
                    vector=v, 
                    payload={"text": c, "source": file_name}
                ) for c, v in zip(chunks, vectors)]
                client.upsert(collection_name=COLLECTION_NAME, points=points)
                print(f" âœ… ({len(chunks)} å€å¡Š)")
            else:
                print(" âŒ å‘é‡åŒ–å¤±æ•—")

# === 4. åŸ·è¡Œ RAG ä»»å‹™ (ä¿®æ­£å‹åˆ¥è¡çªèˆ‡ 502 éŒ¯èª¤) ===
def run_rag_task():
    print("\n" + "="*50)
    input_file = "Re_Write_questions.csv" 
    prompt_file = "Prompt_ReWrite.txt"
    output_file = "Re_Write_questions_result.csv"

    if not os.path.exists(input_file) or not os.path.exists(prompt_file):
        print("âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°å¿…è¦æª”æ¡ˆã€‚")
        return

    with open(prompt_file, "r", encoding="utf-8") as f:
        rewrite_instruction = f.read()

    # è®€å– CSV
    df = pd.read_csv(input_file, encoding='utf-8-sig')
    df.columns = df.columns.str.strip()
    
    # --- é—œéµä¿®æ­£ï¼šé å…ˆå¼·åˆ¶è½‰æ›å‹åˆ¥ç‚ºå­—ä¸²ç‰©ä»¶ï¼Œé¿å… LossySetitemError ---
    df['answer'] = ""
    df['answer'] = df['answer'].astype(object)
    df['source'] = ""
    df['source'] = df['source'].astype(object)
    # -------------------------------------------------------------

    session_history = {} 
    print(f"ğŸš€ [æ­¥é©Ÿ 2/2] é–‹å§‹è™•ç†å•é¡Œé›† (å…± {len(df)} é¡Œ)...")

    for index, row in df.iterrows():
        cid = str(row['conversation_id'])
        original_q = str(row['questions']) 
        history = session_history.get(cid, "")

        print(f"\n--- [æ­£åœ¨è™•ç†ç¬¬ {index+1} é¡Œ] (CID: {cid}) ---")

        # A. å•é¡Œæ”¹å¯« (å«ç°¡å–®é‡è©¦)
        rewritten_q = original_q
        for _ in range(2):
            try:
                rewrite_prompt = f"{rewrite_instruction}\n\n[æ­·å²]:\n{history}\n\n[å•é¡Œ]:\n{original_q}\n\næœå°‹å¥ï¼š"
                rewritten_q = llm.invoke(rewrite_prompt).content.strip()
                print(f"ğŸ” æœå°‹å¥: {rewritten_q}")
                break
            except: time.sleep(2)

        # B. æª¢ç´¢
        q_vec_list = get_embeddings_batch([rewritten_q])
        context, top_source = "", "æœªçŸ¥"
        if q_vec_list:
            q_vec = q_vec_list[0]
            hits = client.query_points(collection_name=COLLECTION_NAME, query=q_vec, limit=3).points
            context = "\n".join([h.payload['text'] for h in hits])
            top_source = hits[0].payload['source'] if hits else "æœªçŸ¥ä¾†æº"
            for i, hit in enumerate(hits):
                print(f"  ğŸ“ åŒ¹é…é … {i+1}: {hit.payload['text'][:30]}...")

        # C. å›ç­”ç”Ÿæˆ (è™•ç† 502 Bad Gateway)
        final_prompt = (
            f"ä½ æ˜¯ä¸€å€‹åŠ©æ‰‹ï¼Œè«‹æ ¹æ“šè³‡è¨Šå›ç­”å•é¡Œã€‚è«‹ä½¿ç”¨æ­£ç¢ºçš„ç¹é«”ä¸­æ–‡ï¼Œé¿å…éŒ¯å­—ã€‚\n"
            f"è‹¥è³‡è¨Šä¸­å‡ºç¾ç·¨ç¢¼åç§»ï¼ˆå¦‚ã€è™¨æ“¬ã€ã€ã€æ–§ç†å™¨ã€ï¼‰ï¼Œè«‹è‡ªå‹•ä¿®æ­£ç‚ºæ­£ç¢ºåè©ï¼ˆå¦‚ã€è™›æ“¬ã€ã€ã€è™•ç†å™¨ã€ï¼‰ã€‚\n\n"
            f"ã€è³‡è¨Šã€‘ï¼š\n{context}\n\n"
            f"ã€å•é¡Œã€‘ï¼š{rewritten_q}\nå›ç­”ï¼š"
        )
        
        answer = "ä¼ºæœå™¨æš«æ™‚é€£ç·šå¤±æ•—ï¼Œè«‹æª¢æŸ¥å¾Œç«¯ç‹€æ…‹ã€‚"
        for attempt in range(3):
            try:
                answer_content = llm.invoke(final_prompt).content.strip().replace('\ufffd', '')
                answer = answer_content
                print(f"âœ¨ AI å›ç­”æˆåŠŸ")
                break
            except Exception as e:
                print(f"  âš ï¸ ç”Ÿæˆå¤±æ•— (å˜—è©¦ {attempt+1})ï¼ŒåŸå› : {e}")
                time.sleep(7) # é‡åˆ° 502/504 æ™‚ï¼Œè®“ä¼ºæœå™¨å–˜æ¯ä¸€ä¸‹

        # å¡«å…¥çµæœ
        df.at[index, 'answer'] = answer
        df.at[index, 'source'] = top_source
        
        # æ›´æ–°å°è©±æ­·å²
        session_history[cid] = history + f"å•ï¼š{original_q}\nç­”ï¼š{answer}\n"

    # è¼¸å‡ºæœ€çµ‚çµæœ
    df.to_csv(output_file, index=False, encoding="utf-8-sig")
    print(f"\n" + "="*50)
    print(f"ğŸ‰ ä»»å‹™è™•ç†å®Œç•¢ï¼çµæœå„²å­˜è‡³: {output_file}")

if __name__ == "__main__":
    initialize_db()
    run_rag_task()