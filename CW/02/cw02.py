import os
import io
import pandas as pd
import requests
import uuid
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter

# === 1. åˆå§‹åŒ–èˆ‡ VDB è¨­å®š ===
client = QdrantClient(url="http://localhost:6333")

MODES = {
    "COSINE": {"name": "hw_final_cosine", "dist": Distance.COSINE},
    "DOT": {"name": "hw_final_dot", "dist": Distance.DOT},
    "EUCLID": {"name": "hw_final_euclid", "dist": Distance.EUCLID}
}

EMBED_API_URL = "https://ws-04.wade0426.me/embed"

def get_embeddings(texts):
    payload = {"texts": texts, "normalize": True, "batch_size": 32}
    try:
        response = requests.post(EMBED_API_URL, json=payload)
        response.raise_for_status()
        return response.json()['embeddings']
    except Exception as e:
        print(f"âŒ Embedding API éŒ¯èª¤: {e}")
        return []

# === 2. å¯¦ä½œåˆ‡å¡Šå°æ¯”å°å‡º (ä»»å‹™ 2 & 3) ===

def perform_dual_chunking(file_path):
    if not os.path.exists(file_path):
        print(f"âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆ: {file_path}")
        return [], []

    with open(file_path, "r", encoding="utf-8") as f:
        text = f.read()

    # å›ºå®šåˆ‡å¡Š
    fixed_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0, separator="")
    fixed_chunks = [doc.page_content for doc in fixed_splitter.create_documents([text])]

    # æ»‘å‹•è¦–çª—åˆ‡å¡Š
    sliding_splitter = RecursiveCharacterTextSplitter(
        separators=["\n\n", "\n", "ã€‚ ", "! ", "? ", " ", ""],
        chunk_size=100,
        chunk_overlap=30, 
        add_start_index=True
    )
    sliding_chunks = [doc.page_content for doc in sliding_splitter.create_documents([text])]
    
    print("\n" + "="*20 + " ã€2. å›ºå®šåˆ‡å¡Šè©³ç´°å…§å®¹ (Fixed)ã€‘ " + "="*20)
    for i, c in enumerate(fixed_chunks):
        # ä¿®æ­£ï¼šå…ˆè™•ç†å­—ä¸²ï¼Œé¿å… f-string åæ–œç·šéŒ¯èª¤
        clean_text = c.replace('\n', ' ')
        print(f"Chunk {i+1}: {clean_text}")
        
    print("\n" + "="*20 + " ã€3. æ»‘å‹•è¦–çª—è©³ç´°å…§å®¹ (Sliding)ã€‘ " + "="*20)
    for i, c in enumerate(sliding_chunks):
        clean_text = c.replace('\n', ' ')
        print(f"Chunk {i+1}: {clean_text}")
    
    return fixed_chunks, sliding_chunks

# === 3. è¡¨æ ¼è™•ç†éç¨‹èˆ‡çµæœå°å‡º (ä»»å‹™ 6) ===

def process_table_folder(folder_path):
    all_table_data = []
    
    # å„ªåŒ–å¾Œçš„ Prompts
    p1_optimized = "# Role: å•†æ¥­é¡§å•æ‘˜è¦\n# Task: è­˜åˆ¥æ ¡å€ç‰¹è‰²èˆ‡æ——è‰¦è¨ˆç•«è¶¨å‹¢...\n# Input:"
    p2_optimized = "# Role: QA ç”ŸæˆåŠ©ç†\n# Task: ç”Ÿæˆæ¨¡æ“¬çœŸå¯¦ä½¿ç”¨è€…å£å»çš„å•ç­”å°...\n# Input:"

    print("\n" + "="*20 + " ã€è¡¨æ ¼è™•ç†éç¨‹èˆ‡çµåˆçµæœã€‘ " + "="*20)
    
    if not os.path.exists(folder_path):
        print(f"âš ï¸ æ‰¾ä¸åˆ°è³‡æ–™å¤¾: {folder_path}")
        return []

    for file_name in os.listdir(folder_path):
        f_path = os.path.join(folder_path, file_name)
        
        # A. HTML è¡¨æ ¼ -> çµåˆå„ªåŒ–å¾Œçš„ Prompt V1 (æ‘˜è¦ä»»å‹™)
        if file_name == "table_html.html":
            with open(f_path, "r", encoding="utf-8") as f:
                html_io = io.StringIO(f.read())
                dfs = pd.read_html(html_io)
                for df in dfs:
                    processed_text = f"{p1_optimized}\n{df.to_string()}"
                    all_table_data.append(processed_text)
                    print(f"\n[è™•ç†æª”æ¡ˆ: {file_name}]\n{processed_text}")

        # B. MD è¡¨æ ¼ -> çµåˆå„ªåŒ–å¾Œçš„ Prompt V2 (QA ä»»å‹™)
        elif file_name == "table_txt.md":
            with open(f_path, "r", encoding="utf-8") as f:
                md_text = f.read()
                processed_text = f"{p2_optimized}\n{md_text}"
                all_table_data.append(processed_text)
                print(f"\n[è™•ç†æª”æ¡ˆ: {file_name}]\n{processed_text}")
            
    return all_table_data

# === 4. åµŒå…¥ VDB (ä½¿ç”¨ UUID) ===

def upsert_to_vdb(chunks, category):
    if not chunks: return
    vectors = get_embeddings(chunks)
    if not vectors: return
    
    for mode, info in MODES.items():
        if not client.collection_exists(info["name"]):
            client.create_collection(
                collection_name=info["name"],
                vectors_config=VectorParams(size=len(vectors[0]), distance=info["dist"])
            )
        
        # UUID ç”Ÿæˆå”¯ä¸€ ID
        points = [
            PointStruct(
                id=uuid.uuid4().hex, 
                vector=vectors[i], 
                payload={"text": chunks[i], "category": category}
            ) for i in range(len(chunks))
        ]
        client.upsert(collection_name=info["name"], points=points)
    print(f"\nâœ… {category} æ•¸æ“šå·²ä½¿ç”¨ UUID å­˜å…¥ Qdrantã€‚")

# === ä¸»ç¨‹å¼åŸ·è¡Œ ===

if __name__ == "__main__":
    # 1. åˆ‡å¡Šå°æ¯”å…§å®¹å°å‡º
    _, sliding_chunks = perform_dual_chunking("text.txt")
    
    # 2. è¡¨æ ¼çµåˆ Prompt éç¨‹å°å‡º
    table_results = process_table_folder("table")
    
    # 3. å„²å­˜
    if sliding_chunks:
        upsert_to_vdb(sliding_chunks, "text_data")
    if table_results:
        upsert_to_vdb(table_results, "table_data")
    
    print("\nğŸš€ ç¨‹å¼åŸ·è¡Œå®Œç•¢ï¼æ‰€æœ‰è™•ç†ç´°ç¯€å·²é¡¯ç¤ºã€‚")