import os
import io
import pandas as pd
import requests
import uuid
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain_openai import ChatOpenAI

# === 0. åˆå§‹åŒ– LLM ===
llm = ChatOpenAI(
    base_url="https://ws-05.huannago.com/v1",
    api_key="YOUR_API_KEY", # âš ï¸ è«‹åœ¨æ­¤å¡«å…¥æ‚¨çš„ API Key
    model="google/gemma-3-27b-it",
    temperature=0.7
)

# === 1. åˆå§‹åŒ–èˆ‡ VDB è¨­å®š ===
client = QdrantClient(url="http://localhost:6333")

MODES = {
    "COSINE": {"name": "hw_final_cosine", "dist": Distance.COSINE},
    "DOT": {"name": "hw_final_dot", "dist": Distance.DOT},
    "EUCLID": {"name": "hw_final_euclid", "dist": Distance.EUCLID}
}

EMBED_API_URL = "https://ws-04.wade0426.me/embed"

def get_embeddings(texts):
    payload = {"texts": texts, "normalize": True, "batch_size": 32}
    try:
        response = requests.post(EMBED_API_URL, json=payload)
        response.raise_for_status()
        return response.json()['embeddings']
    except Exception as e:
        print(f"âŒ Embedding API éŒ¯èª¤: {e}")
        return []

# === 2. å¯¦ä½œæ–‡å­—åˆ‡å¡Šå°æ¯”å°å‡º (text.txt) ===

def perform_dual_chunking(file_path):
    if not os.path.exists(file_path):
        print(f"âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆ: {file_path}")
        return [], []

    with open(file_path, "r", encoding="utf-8") as f:
        text = f.read()

    # å›ºå®šåˆ‡å¡Š
    fixed_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0, separator="")
    fixed_chunks = [doc.page_content for doc in fixed_splitter.create_documents([text])]

    # æ»‘å‹•è¦–çª—åˆ‡å¡Š
    sliding_splitter = RecursiveCharacterTextSplitter(
        separators=["\n\n", "\n", "ã€‚ ", "! ", "? ", " ", ""],
        chunk_size=100,
        chunk_overlap=30, 
        add_start_index=True
    )
    sliding_chunks = [doc.page_content for doc in sliding_splitter.create_documents([text])]
    
    print("\n" + "="*20 + " ã€2. text.txt å›ºå®šåˆ‡å¡Š (Fixed)ã€‘ " + "="*20)
    for i, c in enumerate(fixed_chunks):
        clean_text = c.replace('\n', ' ')
        print(f"Chunk {i+1}: {clean_text}")
        
    print("\n" + "="*20 + " ã€3. text.txt æ»‘å‹•è¦–çª— (Sliding)ã€‘ " + "="*20)
    for i, c in enumerate(sliding_chunks):
        clean_text = c.replace('\n', ' ')
        print(f"Chunk {i+1}: {clean_text}")
    
    return fixed_chunks, sliding_chunks

# === 3. è¡¨æ ¼è™•ç†ï¼šLLM è½‰æ›èˆ‡ç”Ÿæˆå¾Œåˆ‡å¡Š ===

def process_table_via_llm_and_chunk(folder_path):
    """è®€å–è¡¨æ ¼ï¼Œäº¤çµ¦ LLM ç”Ÿæˆæ–‡å­—è³‡è¨Šï¼Œå†é€²è¡Œåˆ‡å¡Š"""
    # è®€å–æœ¬åœ° Prompt æª”æ¡ˆ
    p1_path = os.path.join(folder_path, "Prompt_table_v1.txt")
    p2_path = os.path.join(folder_path, "Prompt_table_v2.txt")
    p1_prompt = open(p1_path, "r", encoding="utf-8").read() if os.path.exists(p1_path) else "è«‹æ‘˜è¦æ­¤è¡¨æ ¼"
    p2_prompt = open(p2_path, "r", encoding="utf-8").read() if os.path.exists(p2_path) else "è«‹æ ¹æ“šæ­¤è¡¨æ ¼ç”Ÿæˆå•ç­”"

    all_llm_chunks = []
    
    # ä¿®æ­£è™•ï¼šæ­£ç¢ºçš„åç¨±ç‚º RecursiveCharacterTextSplitter
    table_text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=150, 
        chunk_overlap=20, 
        separators=["\n\n", "\n", "ã€‚", " "]
    )

    print("\n" + "="*20 + " ã€LLM è¡¨æ ¼è™•ç†èˆ‡åˆ‡å¡Šéç¨‹ã€‘ " + "="*20)
    
    if not os.path.exists(folder_path):
        print(f"âš ï¸ æ‰¾ä¸åˆ°è³‡æ–™å¤¾: {folder_path}")
        return []

    for file_name in os.listdir(folder_path):
        f_path = os.path.join(folder_path, file_name)
        llm_response_text = ""

        # A. HTML -> LLM æ‘˜è¦ (V1)
        if file_name == "table_html.html":
            with open(f_path, "r", encoding="utf-8") as f:
                html_io = io.StringIO(f.read())
                dfs = pd.read_html(html_io)
                for df in dfs:
                    input_content = f"{p1_prompt}\nè¡¨æ ¼æ•¸æ“šï¼š\n{df.to_string()}"
                    print(f"æ­£åœ¨è«‹æ±‚ LLM ç”Ÿæˆ {file_name} çš„æ‘˜è¦å ±å‘Š...")
                    response = llm.invoke(input_content)
                    llm_response_text = response.content

        # B. Markdown -> LLM QA (V2)
        elif file_name == "table_txt.md":
            with open(f_path, "r", encoding="utf-8") as f:
                md_text = f.read()
                input_content = f"{p2_prompt}\nè¡¨æ ¼æ•¸æ“šï¼š\n{md_text}"
                print(f"æ­£åœ¨è«‹æ±‚ LLM ç”Ÿæˆ {file_name} çš„å•ç­”æ•¸æ“š...")
                response = llm.invoke(input_content)
                llm_response_text = response.content
        
        # è™•ç† LLM ç”¢å‡ºçš„æ–‡å­—ä¸¦åˆ‡å¡Š
        if llm_response_text:
            print(f"\n--- LLM ç”Ÿæˆå…§å®¹ ({file_name}) ---\n{llm_response_text}\n")
            
            # å° LLM çš„é•·å›ç­”é€²è¡Œåˆ‡å¡Šï¼Œä»¥ä¾¿æ›´å¥½çš„æª¢ç´¢
            chunks = [doc.page_content for doc in table_text_splitter.create_documents([llm_response_text])]
            
            print(f"--- LLM å…§å®¹åˆ‡å¡Šçµæœ ({file_name}) ---")
            for i, chunk in enumerate(chunks):
                clean_chunk = chunk.replace('\n', ' ')
                print(f"LLM_Chunk {i+1}: {clean_chunk}")
                all_llm_chunks.append(chunk)
            
    return all_llm_chunks

# === 4. åµŒå…¥ VDB (UUID) ===

def upsert_to_vdb(chunks, category):
    if not chunks: return
    vectors = get_embeddings(chunks)
    if not vectors: return
    
    for mode, info in MODES.items():
        if not client.collection_exists(info["name"]):
            client.create_collection(
                collection_name=info["name"],
                vectors_config=VectorParams(size=len(vectors[0]), distance=info["dist"])
            )
        # ä½¿ç”¨ UUID
        points = [
            PointStruct(id=uuid.uuid4().hex, vector=vectors[i], payload={"text": chunks[i], "category": category}) 
            for i in range(len(chunks))
        ]
        client.upsert(collection_name=info["name"], points=points)
    print(f"\nâœ… {category} æ•¸æ“šå·²æˆåŠŸå­˜å…¥ Qdrantã€‚")

# === ä¸»ç¨‹å¼ ===

if __name__ == "__main__":
    # 1. è™•ç†åŸå§‹æ–‡å­—
    _, sliding_text = perform_dual_chunking("text.txt")
    
    # 2. é€é LLM è™•ç†è¡¨æ ¼ä¸¦åˆ‡å¡Š
    llm_chunks = process_table_via_llm_and_chunk("table")
    
    # 3. å„²å­˜è‡³è³‡æ–™åº«
    if sliding_text:
        upsert_to_vdb(sliding_text, "text_data")
    if llm_chunks:
        upsert_to_vdb(llm_chunks, "llm_enhanced_table_data")
    
    print("\nğŸš€ ä»»å‹™å®Œæˆï¼LLM ç”Ÿæˆçš„å…§å®¹å·²æˆåŠŸåˆ‡å¡Šä¸¦å„²å­˜ã€‚")